{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask A: Entity and Relationship Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install spacy\n",
    "#pip install nltk\n",
    "#pip install torch\n",
    "#pip install requests beautifulsoup4\n",
    "#pip install yahooquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import spacy \n",
    "import nltk\n",
    "import requests \n",
    "import torch\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = 'data/ecmdatabase.db'\n",
    "con = sqlite3.connect(f\"file:{dbpath}?mode=ro\", uri=True)\n",
    "with con:\n",
    "    result = con.execute(\"SELECT item1 from companies WHERE stock_symbol = 'TSLA';\")\n",
    "    tsla_item1 = result.fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_item1 = tsla_item1.replace('\\n', '')\n",
    "tsla_item1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration - Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FREQUENCY ANALYSIS\n",
    "from collections import Counter\n",
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.disable_pipe(\"parser\")\n",
    "nlp.enable_pipe(\"senter\")\n",
    "# load data\n",
    "doc = nlp(tsla_item1)\n",
    "words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "print(Counter(words).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Step 1: Load and preprocess the text\n",
    "text_data = [tsla_item1]\n",
    "\n",
    "# Tokenization using nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenization and lowercasing\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the data\n",
    "processed_text = [preprocess(text) for text in text_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10)  # Limit to top 10 features for brevity\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_text)\n",
    "tfidf_keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top TF-IDF Keywords:\")\n",
    "print(tfidf_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LDA\n",
    "tokenized_texts = [preprocess(text).split() for text in text_data]\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "\n",
    "# Display topics\n",
    "print(\"LDA Topics:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging using spaCy\n",
    "doc = nlp(tsla_item1)\n",
    "\n",
    "# Extract POS tags\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "print(\"Part-of-Speech Tags:\")\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using nltk\n",
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "''' \n",
    "# Step Two: Load Data\n",
    "print(len(tsla_item1))\n",
    "\n",
    "# Step Three: Tokenise, find parts of speech and chunk words \n",
    "nltk_name=[]\n",
    "nltk_label=[]\n",
    "for sent in nltk.sent_tokenize(tsla_item1):\n",
    "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "     if hasattr(chunk, 'label'):\n",
    "      entity_name = ' '.join(c[0] for c in chunk)\n",
    "      entity_label = chunk.label()\n",
    "      nltk_name.append(entity_name)\n",
    "      nltk_label.append(entity_label)\n",
    "      print(entity_label, entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using spacy\n",
    "\n",
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.disable_pipe(\"parser\")\n",
    "nlp.enable_pipe(\"senter\")\n",
    "# load data\n",
    "doc = nlp(tsla_item1)\n",
    "Spacy_name=[]\n",
    "Spacy_label=[]\n",
    "# collect unique labels\n",
    "unique_labels = {}\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in unique_labels.keys():\n",
    "        unique_labels[ent.label_] = []\n",
    "    unique_labels[ent.label_].append((ent.text, ent.start_char, ent.end_char))\n",
    "    Spacy_name.append(ent.text)\n",
    "    Spacy_label.append(ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PERSON:      People, including fictional.\n",
    "- NORP:        Nationalities or religious or political groups.\n",
    "- FAC:         Buildings, airports, highways, bridges, etc.\n",
    "- ORG:         Companies, agencies, institutions, etc.\n",
    "- GPE:         Countries, cities, states.\n",
    "- LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW:         Named documents made into laws.\n",
    "- LANGUAGE:    Any named language.\n",
    "- DATE:        Absolute or relative dates or periods.\n",
    "- TIME:        Times smaller than a day.\n",
    "- PERCENT:     Percentage, including ”%“.\n",
    "- MONEY:       Monetary values, including unit.\n",
    "- QUANTITY:    Measurements, as of weight or distance.\n",
    "- ORDINAL:     “first”, “second”, etc.\n",
    "- CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels['ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels['PRODUCT'] #products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels['LOC'] # locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels['FAC'] # facilities / factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels['EVENT'] # events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "#displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because entities can be of different types and not all are equally important in the\n",
    "context of the natural language text being analyzed, it is quite common for NER\n",
    "processors to return the following in addition to a list of entities:\n",
    "\n",
    " **type**\n",
    "- Is it a person? Is it a location? Is it an organization? The set of categories will\n",
    "depend on the specific model used. \n",
    "- The bert-base-NER distinguishes four types\n",
    "of entities: location (LOC), organization (ORG), person (PER), and miscellaneous\n",
    "(MISC).\n",
    " \n",
    "**salience**\n",
    "- The relative importance in the text analyzed or, in other words, the entity’s\n",
    "relevance. \n",
    "- Is the entity central to the text (higher score/salience), or is it just\n",
    "mentioned tangentially (lower score/salience)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipe = pipeline(\"ner\", model = model, tokenizer = tokenizer)\n",
    "for ent in ner_pipe(tsla_item1):\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffbot API\n",
    "https://www.diffbot.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "TOKEN = getpass('Enter token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = \"entities,facts\"\n",
    "HOST = \"nl.diffbot.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_request(payload):\n",
    "  res = requests.post(\"https://{}/v1/?fields={}&token={}\".format(HOST, FIELDS, TOKEN), json=payload)\n",
    "  ret = None\n",
    "  try:\n",
    "    ret = res.json()\n",
    "  except:\n",
    "    print(\"Bad response: \" + res.text)\n",
    "    print(res.status_code)\n",
    "    print(res.headers)\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_request({\n",
    "    \"content\": tsla_item1,\n",
    "    \"lang\": \"en\",\n",
    "    \"format\": \"plain text with title\",\n",
    "})\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Entities and Entity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(res):\n",
    "    for ent in res[\"entities\"]:\n",
    "        if ent[\"salience\"] > 0.5:\n",
    "            print(\"Entity Name: \" + ent['name'])\n",
    "            print(\"Salience: \" + str(ent['salience']))\n",
    "            print(\"Entity Types:\")\n",
    "            print([ent_type[\"name\"] for ent_type in ent['allTypes']])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in res[\"entities\"]:\n",
    "    if ent[\"salience\"] > 0.5:\n",
    "        print(\"Entity Name: \" + ent['name'])\n",
    "        print(\"Salience: \" + str(ent['salience']))\n",
    "        print(\"Entity Types:\")\n",
    "        print([ent_type[\"name\"] for ent_type in ent['allTypes']])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"facts\" in res:\n",
    "    df = pd.DataFrame.from_dict(res[\"facts\"])\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Source: Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_wiki = \"Tesla, Inc. (/ˈtɛslə/ TESS-lə or /ˈtɛzlə/ TEZ-lə[a]) is an American multinational automotive and clean energy company. Headquartered in Austin, Texas, it designs, manufactures and sells battery electric vehicles (BEVs), stationary battery energy storage devices from home to grid-scale, solar panels and solar shingles, and related products and services. \\\n",
    "    Tesla was incorporated in July 2003 by Martin Eberhard and Marc Tarpenning as Tesla Motors. Its name is a tribute to inventor and electrical engineer Nikola Tesla. In February 2004, Elon Musk joined as Tesla's largest shareholder; in 2008, he was named chief executive officer. In 2008, the company began production of its first car model, the Roadster sports car, followed by the Model S sedan in 2012, the Model X SUV in 2015, the Model 3 sedan in 2017, the Model Y crossover in 2020, the Tesla Semi truck in 2022 and the Cybertruck pickup truck in 2023. The Model 3 is the all-time best-selling plug-in electric car worldwide, and in June 2021 became the first electric car to sell 1 million units globally.[6] In 2023, the Model Y was the best-selling vehicle, of any kind, globally.[7][8][3] \\\n",
    "        Tesla is one of the world's most valuable companies in terms of market capitalization. In October 2021, Tesla temporarily became a trillion-dollar company, the seventh U.S. company to do so. In 2023, the company led the battery electric vehicle market, with 19.9% share. Also in 2023, the company was ranked 69th in the Forbes Global 2000.[9] As of March 2024, it is the world's most valuable automaker. Tesla has been the subject of lawsuits, government scrutiny, and journalistic criticism, stemming from allegations of multiple cases of whistleblower retaliation, worker rights violations such as sexual harassment and anti-union activities, safety defects leadings to dozens of recalls, the lack of a public relations department, and controversial statements from Musk including overpromising on the company's driving assist technology and product release timelines.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_request({\n",
    "    \"content\": tesla_wiki,\n",
    "    \"lang\": \"en\",\n",
    "    \"format\": \"plain text with title\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entities(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"facts\" in res:\n",
    "    df = pd.DataFrame.from_dict(res[\"facts\"])\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.set_option('display.width', 3000)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Entity-Relationship Extraction Pipeline for Wikipedia Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahooquery import Ticker\n",
    "\n",
    "def get_company_name(ticker):\n",
    "    try:\n",
    "        ticker_info = Ticker(ticker)\n",
    "        company_name = ticker_info.quote_type[ticker]['longName']\n",
    "        print(f\"Found Company: {company_name}\")\n",
    "        return company_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching company name for ticker {ticker}: {e}\")\n",
    "        return ticker\n",
    "\n",
    "get_company_name(\"AEP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_wikipedia_article(company_name_or_ticker):\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{company_name_or_ticker}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "\n",
    "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "\n",
    "        paragraphs = content_div.find_all('p')\n",
    "\n",
    "        full_article_text = '\\n\\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "\n",
    "        #print(f\"Title: {title}\")\n",
    "        #print(f\"Full Article:\\n{full_article_text}\")\n",
    "        return f\"{title}\" + \" \" + f\"{full_article_text}\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the Wikipedia article: {e}\")\n",
    "\n",
    "# company_name_or_ticker = input(\"Enter the company name or ticker code: \").replace(' ', '_')\n",
    "# get_wikipedia_article(company_name_or_ticker)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_ner_rel_pipeline(ticker):\n",
    "    company_name = get_company_name(ticker)\n",
    "    article = get_wikipedia_article(company_name)\n",
    "    res = get_request({\n",
    "    \"content\": article,\n",
    "    \"lang\": \"en\",\n",
    "    \"format\": \"plain text with title\",\n",
    "    })\n",
    "    ents, rels = None, None\n",
    "    if \"entities\" in res:\n",
    "        ents = pd.DataFrame.from_dict(res[\"entities\"])\n",
    "    if \"facts\" in res:\n",
    "        rels = pd.DataFrame.from_dict(res[\"facts\"])\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.set_option('display.width', 3000)\n",
    "    \n",
    "    return (ents, rels)\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_rels = wikipedia_ner_rel_pipeline(\"AEP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ents_rels[0])\n",
    "print(ents_rels[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Load spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(tsla_item1)\n",
    "\n",
    "# Extract entities and relations\n",
    "nodes = defaultdict(set)  # Use a set to avoid duplicates\n",
    "edges = []\n",
    "\n",
    "# Define a function to identify non-company keywords\n",
    "def is_non_company(entity_text):\n",
    "    # Keywords or patterns that indicate the entity is not a company\n",
    "    non_company_keywords = [\n",
    "        'program', 'act', 'regulation', 'department', 'agency', 'council',\n",
    "        'commission', 'service', 'policy', 'initiative', 'standard', \n",
    "        'incentive', 'college', 'school', 'university', 'authority', 'board',\n",
    "        'order', 'capital', 'vehicle', 'development', 'internship', 'apprenticeship',\n",
    "        'system', 'training', 'product', 'directive', 'committee', 'resource',\n",
    "        'partnership', 'technology', 'platform'\n",
    "    ]\n",
    "    \n",
    "    # If the entity contains any of these keywords, it is not a company\n",
    "    return any(re.search(r'\\b' + keyword + r'\\b', entity_text.lower()) for keyword in non_company_keywords)\n",
    "\n",
    "# Improved function to categorize and extract entities\n",
    "def categorize_entities(entity):\n",
    "    if entity.label_ == 'ORG':\n",
    "        # Check if it's not a company\n",
    "        if is_non_company(entity.text):\n",
    "            return  # Exclude if it's not a company\n",
    "        nodes['Company'].add(entity.text)\n",
    "    elif entity.label_ == 'PRODUCT':\n",
    "        nodes['Product'].add(entity.text)\n",
    "    elif entity.label_ == 'GPE':\n",
    "        nodes['Country'].add(entity.text)\n",
    "    elif entity.label_ in ['NORP', 'INDUSTRY']:\n",
    "        nodes['Industry'].add(entity.text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    categorize_entities(ent)\n",
    "\n",
    "# Synonyms to match different verbs for edge detection\n",
    "target_phrases = {\n",
    "    'SUPPLIES': [\"provide products\", \"manufactures products\", \"delivers goods\", \"offers items\", \"distributes products\"],\n",
    "    'LOCATED_IN': [\"headquartered in\", \"based in\", \"located in\", \"situated in\"],\n",
    "    'RIVALS_WITH': [\"competes with\", \"is a competitor of\", \"challenges\", \"rivals\"],\n",
    "    'ALLIES_WITH': [\"partners with\", \"collaborates with\", \"is allied with\", \"cooperates with\"]\n",
    "}\n",
    "\n",
    "# Extract relationships using semantic similarity\n",
    "for sent in doc.sents:\n",
    "    sent_vector = sent.vector  # Get the vector of the current sentence\n",
    "    for relationship, phrases in target_phrases.items():\n",
    "        for phrase in phrases:\n",
    "            phrase_vector = nlp(phrase).vector  # Get the vector for the target phrase\n",
    "            similarity = sent.similarity(nlp(phrase))  # Compute similarity\n",
    "            if similarity > 0.7:  # Threshold for determining a match\n",
    "                # Check if entities in the sentence can be matched to the known nodes\n",
    "                for entity in sent.ents:\n",
    "                    if entity.label_ == \"ORG\" and entity.text in nodes[\"Company\"]:\n",
    "                        target_entity = None\n",
    "                        if relationship == \"SUPPLIES\":\n",
    "                            target_entity = [ent.text for ent in sent.ents if ent.label_ == \"PRODUCT\"]\n",
    "                        elif relationship == \"LOCATED_IN\":\n",
    "                            target_entity = [ent.text for ent in sent.ents if ent.label_ == \"GPE\"]\n",
    "                        elif relationship in [\"RIVALS_WITH\", \"ALLIES_WITH\"]:\n",
    "                            target_entity = [ent.text for ent in sent.ents if ent.label_ == \"ORG\" and ent.text != entity.text]\n",
    "                        \n",
    "                        if target_entity:\n",
    "                            for target in target_entity:\n",
    "                                edges.append((entity.text, relationship, target))\n",
    "\n",
    "\n",
    "\n",
    "# Convert nodes to list to remove duplicates and maintain order\n",
    "nodes = {k: list(v) for k, v in nodes.items()}\n",
    "\n",
    "# Output nodes and edges\n",
    "print(\"Nodes:\")\n",
    "for node_type, node_list in nodes.items():\n",
    "    print(f\"{node_type}: {node_list}\")\n",
    "\n",
    "print(\"\\nEdges:\")\n",
    "for edge in edges:\n",
    "    print(f\"{edge[0]} -[{edge[1]}]-> {edge[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(name, label):\n",
    "    name_counts = {}\n",
    "    for n, l in zip(name, label):\n",
    "        if n in name_counts:\n",
    "            name_counts[n] += 1\n",
    "        else:\n",
    "            name_counts[n] = 1\n",
    "    return name_counts\n",
    "\n",
    "def transfer_dic(name, label):\n",
    "    name_label_counts = {}\n",
    "    for n, l in zip(name, label):\n",
    "        key = (n, l) \n",
    "        if key in name_label_counts:\n",
    "            name_label_counts[key] += 1\n",
    "        else:\n",
    "            name_label_counts[key] = 1\n",
    "    return name_label_counts\n",
    "\n",
    "# output first the total appearing counts and the respective counts of each label\n",
    "def check_name(name, label, target_name):\n",
    "    if target_name not in name:\n",
    "        return \"no target_name\"\n",
    "    name_counts=count(name, label)\n",
    "    name_label_counts=transfer_dic(name, label)\n",
    "    print(\"total counts \"+ str(name_counts[target_name]))\n",
    "    for i in name_label_counts:\n",
    "        if i[0]==target_name:\n",
    "            print((i[1],name_label_counts[i]))\n",
    "\n",
    "# output a dictionary containg the key of name and the label with highest appearing ratio. {name:(label,ratio)}\n",
    "def get_NER(name, label):\n",
    "    res={}\n",
    "    name_counts=count(name, label)\n",
    "    name_label_counts=transfer_dic(name, label)\n",
    "    for i in list(set(name)):\n",
    "        for j in name_label_counts:\n",
    "            if j[0]==i:\n",
    "                ratio=name_label_counts[j]/name_counts[i]\n",
    "                if j[0] in res:\n",
    "                    if ratio>res[j[0]][1]:\n",
    "                        res[j[0]]=(j[1],ratio)\n",
    "                else:\n",
    "                    res[j[0]]=(j[1],ratio)\n",
    "    return res\n",
    "\n",
    "def highest_label(name, label):\n",
    "    res={}\n",
    "    NER=get_NER(name, label)\n",
    "    for i in NER:\n",
    "        if NER[i][0] in res:\n",
    "            res[NER[i][0]].append(i)\n",
    "        else:\n",
    "            res[NER[i][0]]=[i,]\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "print(get_NER(Spacy_name,Spacy_label))\n",
    "print(get_NER(nltk_name,nltk_label))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
